# ---- vLLM on V100 (compute 7.0) ----
# Compatible with Tesla V100 GPUs (cc 7.0)
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ARG DEBIAN_FRONTEND=noninteractive
ARG PIP_DISABLE_PIP_VERSION_CHECK=1

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-dev git ca-certificates curl \
    && rm -rf /var/lib/apt/lists/*

# Ensure python and pip point to python3
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1 \
 && update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    TORCH_CUDA_ARCH_LIST="7.0" \
    VLLM_USE_FLASH_ATTENTION="0" \
    VLLM_NO_FLASH_ATTENTION="1" \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# ---- Python packages ----
RUN pip install --upgrade pip setuptools wheel \
 && pip install --extra-index-url https://download.pytorch.org/whl/cu121 \
    torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2

# vLLM (version pinned for compatibility)
ARG VLLM_VERSION=0.3.3
RUN pip install \
    "vllm==${VLLM_VERSION}" \
    "transformers>=4.41,<4.46" \
    "accelerate>=0.29" \
    "huggingface_hub>=0.22" \
    "sentencepiece" \
    "einops"

# Non-root user
ARG USER=app
ARG UID=10001
RUN useradd -m -u ${UID} -s /bin/bash ${USER}
USER ${USER}
WORKDIR /workspace

EXPOSE 8000 8100

# Entrypoint allows Compose to inject runtime args like --model, --host, --port
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--help"]



CFG="config.json"

# backup first
cp "$CFG" "${CFG}.bak"

# set max context = 8192, rope_scaling disabled
jq '.rope_scaling = null | .max_position_embeddings = 8192' "$CFG" > "$CFG.tmp" \
  && mv "$CFG.tmp" "$CFG"



curl http://127.0.0.1:8100/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "/models/Llama-3.1-8B-Instruct",
        "prompt": "What is the capital of France?",
        "max_tokens": 100
      }'


# litellm_config.yaml
model_list:
  - model_name: local-llama          # what clients will use
    litellm_params:
      # talk to vLLM via OpenAI-compatible API
      model: /models/Llama-3.1-8B-Instruct
      custom_llm_provider: openai
      api_base: http://127.0.0.1:8100/v1
      # no real key is needed by vLLM; LiteLLM still expects a header
      api_key: "dummy"

# Optional hardening / QoS
num_retries: 1
timeout: 120
telemetry: false
cache: false



litellm:
    image: ghcr.io/berriai/litellm:latest
    container_name: litellm
    # host networking so it can reach vLLM at 127.0.0.1:8100
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./litellm_config.yaml:/config.yaml:ro
    environment:
      - LITELLM_LOG=info
      # Optional: require a key for clients (static)
      - LITELLM_MASTER_KEY=secret123
    command: >
      litellm --port 9001
              --config /config.yaml
              --host 127.0.0.1
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://127.0.0.1:9001/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5

curl http://127.0.0.1:9001/v1/chat/completions \
 -H "Content-Type: application/json" \
  -H "Authorization: Bearer secret123" \
  -d '{
    "model": "local-llama",
    "messages":[{"role":"user","content":"Summarize vLLM vs LiteLLM in one line."}],
    "max_tokens": 64
  }'


curl -H "Authorization: Bearer secret123" http://127.0.0.1:9001/v1/models

user  nginx;
worker_processes  auto;
events { worker_connections 1024; }

http {
    # sensible defaults
    sendfile on;
    tcp_nopush on;
    server_tokens off;
    client_max_body_size 100M;
    proxy_read_timeout 300s;
    proxy_connect_timeout 10s;
    proxy_send_timeout 300s;

    # upstreams point to host loopback (your services are bound to 127.0.0.1)
    upstream vllm_up {
        server 127.0.0.1:8100;
        keepalive 16;
    }

    upstream litellm_up {
        server 127.0.0.1:9001;
        keepalive 16;
    }

    server {
        listen 80;
        server_name _;

        # Route by path
        location /vllm/ {
            proxy_pass http://vllm_up/;            # preserves relative path
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_buffering off;                   # streaming / low-latency models benefit from this
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        location /litellm/ {
            proxy_pass http://litellm_up/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_buffering off;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        # Health check endpoint for load balancers (optional)
        location /health {
            return 200 'ok';
            add_header Content-Type text/plain;
        }
    }
}


services:
  nginx-proxy:
    image: nginx:stable
    container_name: nginx-proxy
    network_mode: host          # important so nginx can access 127.0.0.1:8100/9001
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped



----
version: '3.9'

services:
  dvllm:
    image: docker.io/library/vllm-custom:v2
    container_name: vllm
    network_mode: host
    restart: unless-stopped
    volumes:
      - ${MODEL_DIR}:/models:Z
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >
      --model /models/${MODEL_NAME}
      --max-model-len 8192
      --host 127.0.0.1
      --port 8100
      --dtype half

  litellm:
    image: bosdoohst01.genisis.va.gov:5000/litellm/litellm:v1.75.8-stable
    container_name: litellm
    network_mode: host
    restart: unless-stopped
    volumes:
      - /litellm_config.yaml:/config.yaml:ro
      - /var/log/litellm:/app/logs              # Mount for log files
    environment:
      - LITELLM_LOG=DEBUG                        # Enable debug logging
      - LITELLM_MASTER_KEY=secret123
      - JSON_LOGS=true                           # Enable JSON formatted logs
    command: >
      bash -c "
      mkdir -p /app/logs &&
      python3 -m litellm
      --port 9001
      --config /config.yaml
      --host 127.0.0.1
      --detailed_debug
      2>&1 | tee -a /app/logs/litellm.log
      "
    logging:
      driver: "json-file"
      options:
        max-size: "50m"                          # Max size per log file
        max-file: "5"                            # Keep 5 rotated files
        compress: "true"                         # Compress old logs

  nginx-proxy:
    image: docker.io/library/nginx:latest
    container_name: nginx-proxy
    network_mode: host
    restart: unless-stopped
    volumes:
      - /nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - dvllm
      - litellm
----
model_list:
  - model_name: custom-model
    litellm_params:
      model: vllm/models/${MODEL_NAME}
      api_base: http://127.0.0.1:8100

general_settings:
  master_key: secret123
  
  # Enable JSON formatted logs
  json_logs: true
  
  # Optional: Store logs in database (requires DATABASE_URL)
  # store_prompts_in_spend_logs: true
  
litellm_settings:
  # Enable verbose logging
  set_verbose: true
  
  # Don't drop parameters (log everything)
  drop_params: false
  
  # Optional: Success/failure callbacks for external logging
  # success_callback: ["langfuse", "s3"]
  # failure_callback: ["sentry"]
---


openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./openwebui_data:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://127.0.0.1:9001/v1
      - OPENAI_API_KEY=secret123  # matches your LITELLM_MASTER_KEY
      - WEBUI_AUTH=false  # set to true if you want login
    depends_on:
      - litellm




---
#!/bin/bash

set -e

AZURE_REPO="$HOME/genhub-repos/genhub"
GITHUB_REPO="$HOME/genhub_aws"
BRANCH="dev"

echo "ðŸ”„ Rsync '$BRANCH' from Azure DevOps â†’ GitHub"

# --- Sanity checks ---
if [ ! -d "$AZURE_REPO/.git" ]; then
  echo "âŒ Azure repo not found: $AZURE_REPO"
  exit 1
fi

if [ ! -d "$GITHUB_REPO/.git" ]; then
  echo "âŒ GitHub repo not found: $GITHUB_REPO"
  exit 1
fi

# --- Azure repo: pull latest dev ---
cd "$AZURE_REPO"

if ! git diff --quiet || ! git diff --cached --quiet; then
  echo "âŒ Azure repo has uncommitted changes. Aborting."
  exit 1
fi

echo "ðŸ“¥ Checking out '$BRANCH' in Azure repo"
git checkout "$BRANCH"

echo "â¬‡ï¸ Pulling latest '$BRANCH' from Azure DevOps"
git pull origin "$BRANCH"

# --- GitHub repo: checkout dev ---
cd "$GITHUB_REPO"

echo "ðŸ“¥ Checking out '$BRANCH' in GitHub repo"
git checkout "$BRANCH" 2>/dev/null || git checkout -b "$BRANCH"

# --- Rsync files ---
echo "ðŸš€ Rsyncing files..."

rsync -av --delete \
  --exclude ".git" \
  --exclude ".gitignore" \
  --exclude "node_modules" \
  --exclude ".DS_Store" \
  "$AZURE_REPO/" "$GITHUB_REPO/"

echo "âœ… Rsync complete"

# --- Show status ---
echo "ðŸ“¦ GitHub repo status:"
git status

