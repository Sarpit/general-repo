# ---- vLLM on V100 (compute 7.0) ----
# Compatible with Tesla V100 GPUs (cc 7.0)
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

ARG DEBIAN_FRONTEND=noninteractive
ARG PIP_DISABLE_PIP_VERSION_CHECK=1

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip python3-dev git ca-certificates curl \
    && rm -rf /var/lib/apt/lists/*

# Ensure python and pip point to python3
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1 \
 && update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    TORCH_CUDA_ARCH_LIST="7.0" \
    VLLM_USE_FLASH_ATTENTION="0" \
    VLLM_NO_FLASH_ATTENTION="1" \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# ---- Python packages ----
RUN pip install --upgrade pip setuptools wheel \
 && pip install --extra-index-url https://download.pytorch.org/whl/cu121 \
    torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2

# vLLM (version pinned for compatibility)
ARG VLLM_VERSION=0.3.3
RUN pip install \
    "vllm==${VLLM_VERSION}" \
    "transformers>=4.41,<4.46" \
    "accelerate>=0.29" \
    "huggingface_hub>=0.22" \
    "sentencepiece" \
    "einops"

# Non-root user
ARG USER=app
ARG UID=10001
RUN useradd -m -u ${UID} -s /bin/bash ${USER}
USER ${USER}
WORKDIR /workspace

EXPOSE 8000 8100

# Entrypoint allows Compose to inject runtime args like --model, --host, --port
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--help"]



CFG="config.json"

# backup first
cp "$CFG" "${CFG}.bak"

# set max context = 8192, rope_scaling disabled
jq '.rope_scaling = null | .max_position_embeddings = 8192' "$CFG" > "$CFG.tmp" \
  && mv "$CFG.tmp" "$CFG"



curl http://127.0.0.1:8100/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "/models/Llama-3.1-8B-Instruct",
        "prompt": "What is the capital of France?",
        "max_tokens": 100
      }'


# litellm_config.yaml
model_list:
  - model_name: local-llama          # what clients will use
    litellm_params:
      # talk to vLLM via OpenAI-compatible API
      model: /models/Llama-3.1-8B-Instruct
      custom_llm_provider: openai
      api_base: http://127.0.0.1:8100/v1
      # no real key is needed by vLLM; LiteLLM still expects a header
      api_key: "dummy"

# Optional hardening / QoS
num_retries: 1
timeout: 120
telemetry: false
cache: false



litellm:
    image: ghcr.io/berriai/litellm:latest
    container_name: litellm
    # host networking so it can reach vLLM at 127.0.0.1:8100
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./litellm_config.yaml:/config.yaml:ro
    environment:
      - LITELLM_LOG=info
      # Optional: require a key for clients (static)
      - LITELLM_MASTER_KEY=secret123
    command: >
      litellm --port 9001
              --config /config.yaml
              --host 127.0.0.1
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://127.0.0.1:9001/v1/models"]
      interval: 15s
      timeout: 5s
      retries: 5

curl http://127.0.0.1:9001/v1/chat/completions \
 -H "Content-Type: application/json" \
  -H "Authorization: Bearer secret123" \
  -d '{
    "model": "local-llama",
    "messages":[{"role":"user","content":"Summarize vLLM vs LiteLLM in one line."}],
    "max_tokens": 64
  }'


curl -H "Authorization: Bearer secret123" http://127.0.0.1:9001/v1/models

user  nginx;
worker_processes  auto;
events { worker_connections 1024; }

http {
    # sensible defaults
    sendfile on;
    tcp_nopush on;
    server_tokens off;
    client_max_body_size 100M;
    proxy_read_timeout 300s;
    proxy_connect_timeout 10s;
    proxy_send_timeout 300s;

    # upstreams point to host loopback (your services are bound to 127.0.0.1)
    upstream vllm_up {
        server 127.0.0.1:8100;
        keepalive 16;
    }

    upstream litellm_up {
        server 127.0.0.1:9001;
        keepalive 16;
    }

    server {
        listen 80;
        server_name _;

        # Route by path
        location /vllm/ {
            proxy_pass http://vllm_up/;            # preserves relative path
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_buffering off;                   # streaming / low-latency models benefit from this
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        location /litellm/ {
            proxy_pass http://litellm_up/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_buffering off;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
        }

        # Health check endpoint for load balancers (optional)
        location /health {
            return 200 'ok';
            add_header Content-Type text/plain;
        }
    }
}


services:
  nginx-proxy:
    image: nginx:stable
    container_name: nginx-proxy
    network_mode: host          # important so nginx can access 127.0.0.1:8100/9001
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped





